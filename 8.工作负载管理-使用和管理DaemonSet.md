# 8.工作负载管理-使用和管理DaemonSet
当谈到 Kubernetes 中的 DaemonSet 时，它是一种控制器类型，确保特定的 Pod 在集群中的所有（或一部分）节点上运行。"DaemonSet" 这个术语本身意味着它确保一个 Pod 的副本，或者说是守护进程，在每个节点上都在运行。这对于部署需要在每个节点上运行的系统级守护进程或后台任务非常有用，例如日志收集器、监控代理或存储守护进程。
## 基本组件
**Pod 模板**：与 Kubernetes 中的其他控制器类似，DaemonSet 使用一个 Pod 模板来创建新的 Pod。该模板定义了将在每个节点上部署的 Pod 的特性。

**节点亲和性**：DaemonSet 可以配置节点亲和性规则，这些规则确定 DaemonSet 应该在哪些节点上运行。这允许您仅将 DaemonSet 部署到符合特定条件的节点上。

**滚动更新**：当更新 DaemonSet（例如通过更改 Pod 模板或图像版本）时，Kubernetes 会自动执行滚动更新，确保新的 Pod 逐渐部署，同时旧的 Pod 被终止。

**节点移除**：如果从集群中添加或删除节点，DaemonSet 会自动调整，以确保在可用节点上运行指定数量的 Pod。
## 应用场景
**监控和日志收集：** 部署监控代理或日志收集器，确保每个节点都能够收集系统或应用程序的监控数据和日志。

**网络代理：** 在每个节点上运行网络代理，用于路由或过滤网络流量。

**存储守护进程：** 部署存储相关的守护进程，确保每个节点都有相应的存储服务。
## 基本配置和使用
* 定义yaml

> 定义了一个DaemonSet，选择节点节点标签包含 nodeServiceType: elasticsearch进行部署
```
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: daemonset-fluentd-elasticsearch
  namespace: default
  labels:
    k8s-app: fluentd-logging
spec:
  selector:
    matchLabels:
      name: fluentd-elasticsearch
  template:
    metadata:
      labels:
        name: fluentd-elasticsearch
    spec:
      nodeSelector:
        nodeServiceType: elasticsearch
      containers:
      - name: fluentd-elasticsearch
        image: quay.io/fluentd_elasticsearch/fluentd:v2.5.2
        resources:
          limits:
            memory: 200Mi
          requests:
            cpu: 100m
            memory: 200Mi
        volumeMounts:
        - name: varlog
          mountPath: /var/log
      terminationGracePeriodSeconds: 30
      volumes:
      - name: varlog
        hostPath:
          path: /var/log
```
* 给节点打上nodeServiceType: elasticsearch 标签
执行上面的yaml之后，发现只有DaemonSet，并没有真正部署pod，原因是我还没有给任何节点打上nodeServiceType标签

![](media/17088172536060/17094681881886.jpg)
现在给k8s-node2打上标签
```
kubectl label node k8s-node2 nodeServiceType=elasticsearch
```
再执行命令可以看到，pod被调度到k8s-node2部署了
![](media/17088172536060/17094682696630.jpg)
再给k8s-node2打上标签，也能观察到同样的效果
![](media/17088172536060/17094683511420.jpg)
当我们尝试给k8s-master大标签，会发生什么呢?
![](media/17088172536060/17094683999029.jpg)
发现并没有部署在k8s-master,这是为什么呢？我们在后面章节给大家介绍